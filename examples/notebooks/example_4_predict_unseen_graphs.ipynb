{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 4: Predicting samples with unknown labels\n",
    "\n",
    "Here we provide an example of extracting features and training a model on samples with known labels. We then take a secondary dataset without labels and predict their class. \n",
    "\n",
    "Of course, in a real scenario we are unable to predict the accuracy of our unlabelled samples. However, here we know how the data is generated and can confirm that the pipeline works correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "from hcga.io import save_dataset\n",
    "from hcga.graph import Graph, GraphCollection\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "if not Path(\"datasets\").exists():\n",
    "    os.mkdir(\"datasets\")\n",
    "if not Path(\"results\").exists():\n",
    "    os.mkdir(\"results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate synthetic data with known labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this cell is generating some synthetic graph dataset with node features.\n",
    "\n",
    "\n",
    "# defining limits on number of nodes\n",
    "n_min = 20\n",
    "n_max = 50\n",
    "\n",
    "# number of graphs\n",
    "num_g = 100\n",
    "\n",
    "# number of node features - in this example I will generate random node features that aren't useful for classifcation\n",
    "n_nf = 3\n",
    "\n",
    "\n",
    "# empty list of graphs and labels\n",
    "graphs = []\n",
    "labels = []\n",
    "node_features = []\n",
    "\n",
    "\n",
    "# setting limits on probability of edge existing for random graphs\n",
    "p_min = 0.1\n",
    "p_max = 0.5\n",
    "\n",
    "# adding 50 random graphs (label 0)\n",
    "for i in range(int(num_g / 2)):\n",
    "    rand_n = np.random.randint(n_min, n_max)\n",
    "    rand_p = np.random.randint(int(p_min * 100), int(p_max * 100)) / 100\n",
    "\n",
    "    g = nx.fast_gnp_random_graph(rand_n, rand_p)\n",
    "    g.label = 0\n",
    "\n",
    "    node_feat_matrix = np.random.random((rand_n, n_nf))\n",
    "\n",
    "    for i, node in enumerate(g.nodes):\n",
    "        g.nodes[node][\"features\"] = node_feat_matrix[i, :]\n",
    "\n",
    "    # graphs.append(g)\n",
    "\n",
    "    graphs.append(nx.to_numpy_array(g) * 2)\n",
    "\n",
    "    node_features.append(node_feat_matrix)\n",
    "\n",
    "    labels.append(0)\n",
    "\n",
    "# setting limits on number of edges to add per node\n",
    "m_min = 1\n",
    "m_max = 5\n",
    "\n",
    "# adding 50  powerlaw cluster graphs (label 1)\n",
    "for i in range(int(num_g / 2)):\n",
    "    rand_n = np.random.randint(n_min, n_max)\n",
    "    rand_p = np.random.randint(int(p_min * 100), int(p_max * 100)) / 100\n",
    "    rand_m = np.random.randint(m_min, m_max)\n",
    "\n",
    "    g = nx.powerlaw_cluster_graph(rand_n, rand_m, rand_p)\n",
    "    g.label = 1\n",
    "\n",
    "    node_feat_matrix = np.random.random((rand_n, n_nf))\n",
    "\n",
    "    for i, node in enumerate(g.nodes):\n",
    "        g.nodes[node][\"features\"] = node_feat_matrix[i, :]\n",
    "\n",
    "    # graphs.append(g)\n",
    "\n",
    "    graphs.append(nx.to_numpy_array(g) * 2)\n",
    "    node_features.append(node_feat_matrix)\n",
    "\n",
    "    labels.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 100 graphs\n",
      "There are 3 features per node\n"
     ]
    }
   ],
   "source": [
    "from hcga.graph import Graph, GraphCollection\n",
    "\n",
    "# create graph collection object\n",
    "graphs_labelled = GraphCollection()\n",
    "graphs_labelled.add_graph_list(graphs, node_features, labels)\n",
    "\n",
    "save_dataset(\n",
    "    graphs_labelled, \"custom_dataset_classification_labelled\", folder=\"./datasets\"\n",
    ")\n",
    "\n",
    "# perform some sanity checks\n",
    "print(\"There are {} graphs\".format(len(graphs_labelled.graphs)))\n",
    "print(\"There are {} features per node\".format(graphs_labelled.get_n_node_features()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract and analyse labelled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define an object\n",
    "from hcga.hcga import Hcga\n",
    "\n",
    "h = Hcga()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:hcga.extraction:Extracting features from 100 graphs (we disabled 0 graphs).\n",
      "INFO:hcga.extraction:Computing features for 100 graphs:\n",
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done   5 tasks      | elapsed:    4.1s\n",
      "[Parallel(n_jobs=4)]: Done  10 tasks      | elapsed:    4.9s\n",
      "[Parallel(n_jobs=4)]: Done  17 tasks      | elapsed:    6.7s\n",
      "[Parallel(n_jobs=4)]: Done  24 tasks      | elapsed:    8.3s\n",
      "[Parallel(n_jobs=4)]: Done  33 tasks      | elapsed:   10.2s\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:   12.5s\n",
      "[Parallel(n_jobs=4)]: Done  53 tasks      | elapsed:   15.1s\n",
      "[Parallel(n_jobs=4)]: Done  64 tasks      | elapsed:   17.6s\n",
      "[Parallel(n_jobs=4)]: Done  77 tasks      | elapsed:   20.3s\n",
      "[Parallel(n_jobs=4)]: Done  90 tasks      | elapsed:   23.1s\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:   25.1s finished\n",
      "INFO:hcga.extraction:1096 feature extracted.\n"
     ]
    }
   ],
   "source": [
    "# load and extract features for the primary dataset with training labels\n",
    "h.load_data(\"./datasets/custom_dataset_classification_labelled.pkl\")\n",
    "h.extract(mode=\"fast\", n_workers=4, timeout=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:hcga.analysis:... Using Xgboost classifier ...\n",
      "INFO:hcga.analysis:1096 total features\n",
      "INFO:hcga.analysis:0 graphs were removed for more than 0.3 fraction of bad features\n",
      "INFO:hcga.analysis:1024 valid features\n",
      "INFO:hcga.analysis:1024 with interpretability 1\n",
      "INFO:hcga.analysis:Counts of graphs/label: \n",
      "0.0    50\n",
      "1.0    50\n",
      "Name: label, dtype: int64\n",
      "INFO:hcga.analysis:Using 10 splits\n",
      "INFO:hcga.analysis:Fold accuracy: --- 1.0 ---\n",
      "INFO:hcga.analysis:Fold accuracy: --- 1.0 ---\n",
      "INFO:hcga.analysis:Fold accuracy: --- 1.0 ---\n",
      "INFO:hcga.analysis:Fold accuracy: --- 1.0 ---\n",
      "INFO:hcga.analysis:Fold accuracy: --- 1.0 ---\n",
      "INFO:hcga.analysis:Fold accuracy: --- 0.9 ---\n",
      "INFO:hcga.analysis:Fold accuracy: --- 1.0 ---\n",
      "INFO:hcga.analysis:Fold accuracy: --- 1.0 ---\n",
      "INFO:hcga.analysis:Fold accuracy: --- 0.9 ---\n",
      "INFO:hcga.analysis:Fold accuracy: --- 1.0 ---\n",
      "INFO:hcga.analysis:Accuracy: 0.98 +/- 0.04\n",
      "INFO:hcga.analysis:Now using a reduced set of 100 features with < 0.9 correlation.\n",
      "INFO:hcga.analysis:Fold accuracy: --- 1.0 ---\n",
      "INFO:hcga.analysis:Fold accuracy: --- 1.0 ---\n",
      "INFO:hcga.analysis:Fold accuracy: --- 1.0 ---\n",
      "INFO:hcga.analysis:Fold accuracy: --- 1.0 ---\n",
      "INFO:hcga.analysis:Fold accuracy: --- 1.0 ---\n",
      "INFO:hcga.analysis:Fold accuracy: --- 0.9 ---\n",
      "INFO:hcga.analysis:Fold accuracy: --- 1.0 ---\n",
      "INFO:hcga.analysis:Fold accuracy: --- 1.0 ---\n",
      "INFO:hcga.analysis:Fold accuracy: --- 0.9 ---\n",
      "INFO:hcga.analysis:Fold accuracy: --- 1.0 ---\n",
      "INFO:hcga.analysis:Accuracy with reduced set: 0.98 +/- 0.04\n",
      "INFO:hcga.analysis:Fitting model to all data\n"
     ]
    }
   ],
   "source": [
    "h.analyse_features(save_model=True, plot=False, results_folder=\"./results/test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct synthetic data with no labels\n",
    "\n",
    "Creating synthetic data constructed in the same way as the training data but without labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this cell is generating some synthetic graph dataset with node features.\n",
    "\n",
    "\n",
    "# defining limits on number of nodes\n",
    "n_min = 20\n",
    "n_max = 50\n",
    "\n",
    "# number of graphs\n",
    "num_g = 20\n",
    "\n",
    "# number of node features - in this example I will generate random node features that aren't useful for classifcation\n",
    "n_nf = 3\n",
    "\n",
    "\n",
    "# empty list of graphs and labels\n",
    "graphs = []\n",
    "node_features = []\n",
    "\n",
    "\n",
    "# setting limits on probability of edge existing for random graphs\n",
    "p_min = 0.1\n",
    "p_max = 0.5\n",
    "\n",
    "# adding 50 random graphs (label 0)\n",
    "for i in range(int(num_g / 2)):\n",
    "    rand_n = np.random.randint(n_min, n_max)\n",
    "    rand_p = np.random.randint(int(p_min * 100), int(p_max * 100)) / 100\n",
    "\n",
    "    g = nx.fast_gnp_random_graph(rand_n, rand_p)\n",
    "    g.label = 0\n",
    "\n",
    "    node_feat_matrix = np.random.random((rand_n, n_nf))\n",
    "\n",
    "    for i, node in enumerate(g.nodes):\n",
    "        g.nodes[node][\"features\"] = node_feat_matrix[i, :]\n",
    "\n",
    "    # graphs.append(g)\n",
    "\n",
    "    graphs.append(nx.to_numpy_array(g) * 2)\n",
    "\n",
    "    node_features.append(node_feat_matrix)\n",
    "\n",
    "\n",
    "# setting limits on number of edges to add per node\n",
    "m_min = 1\n",
    "m_max = 5\n",
    "\n",
    "# adding 50  powerlaw cluster graphs (label 1)\n",
    "for i in range(int(num_g / 2)):\n",
    "    rand_n = np.random.randint(n_min, n_max)\n",
    "    rand_p = np.random.randint(int(p_min * 100), int(p_max * 100)) / 100\n",
    "    rand_m = np.random.randint(m_min, m_max)\n",
    "\n",
    "    g = nx.powerlaw_cluster_graph(rand_n, rand_m, rand_p)\n",
    "    g.label = 1\n",
    "\n",
    "    node_feat_matrix = np.random.random((rand_n, n_nf))\n",
    "\n",
    "    for i, node in enumerate(g.nodes):\n",
    "        g.nodes[node][\"features\"] = node_feat_matrix[i, :]\n",
    "\n",
    "    # graphs.append(g)\n",
    "\n",
    "    graphs.append(nx.to_numpy_array(g) * 2)\n",
    "    node_features.append(node_feat_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 20 graphs in the unlabelled dataset\n",
      "There are 3 features per node\n"
     ]
    }
   ],
   "source": [
    "# create graph collection object\n",
    "graphs_unlabelled = GraphCollection()\n",
    "graphs_unlabelled.add_graph_list(graphs, node_features)  # loaded without the labels\n",
    "\n",
    "# save the unlabelled dataset\n",
    "save_dataset(\n",
    "    graphs_unlabelled, \"custom_dataset_classification_unlabelled\", folder=\"./datasets\"\n",
    ")\n",
    "\n",
    "# perform some sanity checks\n",
    "print(\n",
    "    \"There are {} graphs in the unlabelled dataset\".format(\n",
    "        len(graphs_unlabelled.graphs)\n",
    "    )\n",
    ")\n",
    "print(\"There are {} features per node\".format(graphs_unlabelled.get_n_node_features()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract and predict unlabelled data using pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:hcga.extraction:Extracting features from 20 graphs (we disabled 0 graphs).\n",
      "INFO:hcga.extraction:Computing features for 20 graphs:\n",
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done   5 tasks      | elapsed:    1.6s\n",
      "[Parallel(n_jobs=4)]: Done  10 tasks      | elapsed:    2.7s\n",
      "[Parallel(n_jobs=4)]: Done  16 out of  20 | elapsed:    3.8s remaining:    0.9s\n",
      "[Parallel(n_jobs=4)]: Done  20 out of  20 | elapsed:    4.7s finished\n",
      "INFO:hcga.extraction:1096 feature extracted.\n"
     ]
    }
   ],
   "source": [
    "# extract features for the secondary dataset with no labels\n",
    "h.load_data(\n",
    "    \"./datasets/custom_dataset_classification_unlabelled.pkl\"\n",
    ")  # set prediction graphs to True\n",
    "h.extract(mode=\"fast\", n_workers=4, timeout=20)  # set prediction set to True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:hcga.analysis:1096 total features\n",
      "INFO:hcga.analysis:0 graphs were removed for more than 0.3 fraction of bad features\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"None of [MultiIndex([('EU',   'semi_eulerian'),\\n            ('EU', 'semi_eulerian_E'),\\n            ('EU', 'semi_eulerian_N')],\\n           names=['feature_class', 'feature_name'])] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-9d155fe3e2ef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m h.analyse_features(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mplot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mtrained_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"./results/test/fitted_model\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mresults_folder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"./results/test\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m )\n",
      "\u001b[0;32m~/codes/hcga/hcga/hcga.py\u001b[0m in \u001b[0;36manalyse_features\u001b[0;34m(self, feature_file, results_folder, graph_removal, interpretability, analysis_type, model, compute_shap, kfold, reduce_set, reduced_set_size, reduced_set_max_correlation, plot, max_feats_plot, max_feats_plot_dendrogram, n_repeats, n_splits, random_state, test_size, trained_model, save_model)\u001b[0m\n\u001b[1;32m    159\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeature_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m         analysis(\n\u001b[0m\u001b[1;32m    162\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures_info\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/codes/hcga/hcga/analysis.py\u001b[0m in \u001b[0;36manalysis\u001b[0;34m(features, features_info, graphs, analysis_type, folder, graph_removal, interpretability, model, compute_shap, kfold, reduce_set, reduced_set_size, reduced_set_max_correlation, plot, max_feats_plot, max_feats_plot_dendrogram, n_repeats, n_splits, random_state, test_size, trained_model, save_model)\u001b[0m\n\u001b[1;32m    472\u001b[0m             \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrained_model\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    473\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 474\u001b[0;31m     features, features_info, scaler = _preprocess_features(\n\u001b[0m\u001b[1;32m    475\u001b[0m         \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph_removal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterpretability\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrained_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m     )\n",
      "\u001b[0;32m~/codes/hcga/hcga/analysis.py\u001b[0m in \u001b[0;36m_preprocess_features\u001b[0;34m(features, features_info, graph_removal, interpretability, trained_model)\u001b[0m\n\u001b[1;32m    406\u001b[0m         \u001b[0mpretrained_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature_info_pretrained\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m         \u001b[0mmissing_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpretrained_features\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mpretrained_features\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 408\u001b[0;31m         \u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmissing_cols\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnan\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    409\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmissing_cols\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpretrained_features\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/base/lib/python3.8/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m    686\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m             \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_setitem_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_valid_setitem_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/base/lib/python3.8/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_get_setitem_indexer\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    628\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0msuppress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mIndexingError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 630\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_setter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    631\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/base/lib/python3.8/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_convert_tuple\u001b[0;34m(self, key, is_setter)\u001b[0m\n\u001b[1;32m    752\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_key_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 754\u001b[0;31m                 \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_to_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_setter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_setter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    755\u001b[0m                 \u001b[0mkeyidx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/base/lib/python3.8/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_convert_to_indexer\u001b[0;34m(self, key, axis, is_setter)\u001b[0m\n\u001b[1;32m   1210\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1211\u001b[0m                 \u001b[0;31m# When setting, missing keys are not allowed, even with .loc:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1212\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_listlike_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1213\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1214\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/base/lib/python3.8/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_get_listlike_indexer\u001b[0;34m(self, key, axis, raise_missing)\u001b[0m\n\u001b[1;32m   1264\u001b[0m             \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_indexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reindex_non_unique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1266\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_read_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mraise_missing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1267\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1268\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/base/lib/python3.8/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_validate_read_indexer\u001b[0;34m(self, key, indexer, axis, raise_missing)\u001b[0m\n\u001b[1;32m   1306\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmissing\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m                 \u001b[0maxis_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_axis_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1308\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"None of [{key}] are in the [{axis_name}]\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1309\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1310\u001b[0m             \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"None of [MultiIndex([('EU',   'semi_eulerian'),\\n            ('EU', 'semi_eulerian_E'),\\n            ('EU', 'semi_eulerian_N')],\\n           names=['feature_class', 'feature_name'])] are in the [columns]\""
     ]
    }
   ],
   "source": [
    "h.analyse_features(\n",
    "    plot=False,\n",
    "    trained_model=\"./results/test/fitted_model\",\n",
    "    results_folder=\"./results/test\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lets look at the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pd.read_csv(\"./results/test/prediction_results.csv\", index_col=0)\n",
    "print(predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
