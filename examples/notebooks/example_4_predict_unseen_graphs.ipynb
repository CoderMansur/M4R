{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 4: Predicting samples with unknown labels\n",
    "\n",
    "Here we provide an example of extracting features and training a model on samples with known labels. We then take a secondary dataset without labels and predict their class. \n",
    "\n",
    "Of course, in a real scenario we are unable to predict the accuracy of our unlabelled samples. However, here we know how the data is generated and can confirm that the pipeline works correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "from hcga.io import save_dataset\n",
    "from hcga.graph import Graph, GraphCollection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate synthetic data with known labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this cell is generating some synthetic graph dataset with node features. \n",
    "\n",
    "\n",
    "#defining limits on number of nodes\n",
    "n_min = 20\n",
    "n_max = 50\n",
    "\n",
    "#number of graphs\n",
    "num_g = 100\n",
    "\n",
    "# number of node features - in this example I will generate random node features that aren't useful for classifcation\n",
    "n_nf = 3\n",
    "\n",
    "\n",
    "# empty list of graphs and labels\n",
    "graphs = []\n",
    "labels = []\n",
    "node_features = []\n",
    "\n",
    "\n",
    "# setting limits on probability of edge existing for random graphs\n",
    "p_min = 0.1\n",
    "p_max = 0.5\n",
    "\n",
    "# adding 50 random graphs (label 0)\n",
    "for i in range(int(num_g/2)):\n",
    "    rand_n = np.random.randint(n_min,n_max)\n",
    "    rand_p = np.random.randint(int(p_min*100),int(p_max*100))/100   \n",
    "    \n",
    "    g = nx.fast_gnp_random_graph(rand_n,rand_p)    \n",
    "    g.label = 0\n",
    "\n",
    "    node_feat_matrix = np.random.random((rand_n,n_nf))\n",
    "\n",
    "    for i,node in enumerate(g.nodes):\n",
    "        g.nodes[node]['features'] = node_feat_matrix[i,:]\n",
    "    \n",
    "    #graphs.append(g)\n",
    "    \n",
    "    graphs.append(nx.to_numpy_array(g)*2)\n",
    "    \n",
    "    node_features.append(node_feat_matrix)\n",
    "    \n",
    "    labels.append(0)\n",
    "\n",
    "# setting limits on number of edges to add per node\n",
    "m_min = 1\n",
    "m_max = 5\n",
    "\n",
    "# adding 50  powerlaw cluster graphs (label 1)\n",
    "for i in range(int(num_g/2)):\n",
    "    rand_n = np.random.randint(n_min,n_max)\n",
    "    rand_p = np.random.randint(int(p_min*100),int(p_max*100))/100   \n",
    "    rand_m = np.random.randint(m_min,m_max)\n",
    "    \n",
    "    g = nx.powerlaw_cluster_graph(rand_n, rand_m, rand_p)\n",
    "    g.label = 1\n",
    "\n",
    "    node_feat_matrix = np.random.random((rand_n,n_nf))\n",
    "    \n",
    "    for i,node in enumerate(g.nodes):\n",
    "        g.nodes[node]['features'] = node_feat_matrix[i,:]\n",
    "        \n",
    "    #graphs.append(g)\n",
    "    \n",
    "    graphs.append(nx.to_numpy_array(g)*2)\n",
    "    node_features.append(node_feat_matrix)\n",
    "\n",
    "    labels.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 100 graphs\n",
      "There are 3 features per node\n"
     ]
    }
   ],
   "source": [
    "from hcga.graph import Graph, GraphCollection\n",
    "\n",
    "# create graph collection object\n",
    "graphs_labelled = GraphCollection()\n",
    "graphs_labelled.add_graph_list(graphs,node_features,labels)\n",
    "\n",
    "save_dataset(graphs_labelled, 'custom_dataset_classification_labelled', folder='./datasets')\n",
    "\n",
    "# perform some sanity checks\n",
    "print('There are {} graphs'.format(len(graphs_labelled.graphs)))\n",
    "print('There are {} features per node'.format(graphs_labelled.get_n_node_features()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract and analyse labelled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define an object\n",
    "from hcga.hcga import Hcga\n",
    "\n",
    "h = Hcga()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:hcga.extraction:Extracting features from 100 graphs (we disabled 0 graphs).\n",
      "INFO:hcga.extraction:Computing features for 100 graphs:\n",
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done   5 tasks      | elapsed:    5.5s\n",
      "[Parallel(n_jobs=4)]: Done  10 tasks      | elapsed:    7.5s\n",
      "[Parallel(n_jobs=4)]: Done  17 tasks      | elapsed:   10.0s\n",
      "[Parallel(n_jobs=4)]: Done  24 tasks      | elapsed:   13.2s\n",
      "[Parallel(n_jobs=4)]: Done  33 tasks      | elapsed:   17.0s\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:   20.5s\n",
      "[Parallel(n_jobs=4)]: Done  53 tasks      | elapsed:   25.3s\n",
      "[Parallel(n_jobs=4)]: Done  64 tasks      | elapsed:   29.1s\n",
      "[Parallel(n_jobs=4)]: Done  77 tasks      | elapsed:   33.8s\n",
      "[Parallel(n_jobs=4)]: Done  90 tasks      | elapsed:   38.5s\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:   42.0s finished\n",
      "INFO:hcga.extraction:1093 feature extracted.\n"
     ]
    }
   ],
   "source": [
    "# load and extract features for the primary dataset with training labels\n",
    "h.load_data('./datasets/custom_dataset_classification_labelled.pkl')\n",
    "h.extract(mode='fast', n_workers=4, timeout=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:numexpr.utils:Note: NumExpr detected 12 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "INFO:numexpr.utils:NumExpr defaulting to 8 threads.\n",
      "INFO:hcga.analysis:... Using Xgboost classifier ...\n",
      "INFO:hcga.analysis:1093 total features\n",
      "INFO:hcga.analysis:0 graphs were removed for more than 0.3 fraction of bad features\n",
      "INFO:hcga.analysis:1018 valid features\n",
      "INFO:hcga.analysis:1018 with interpretability 1\n",
      "INFO:hcga.analysis:Counts of graphs/label: \n",
      "0.0    50\n",
      "1.0    50\n",
      "Name: label, dtype: int64\n",
      "INFO:hcga.analysis:Using 10 splits\n",
      "INFO:hcga.analysis:Fold accuracy: --- 1.0 ---\n",
      "INFO:hcga.analysis:Fold accuracy: --- 0.9 ---\n",
      "INFO:hcga.analysis:Fold accuracy: --- 0.9 ---\n",
      "INFO:hcga.analysis:Fold accuracy: --- 1.0 ---\n",
      "INFO:hcga.analysis:Fold accuracy: --- 0.9 ---\n",
      "INFO:hcga.analysis:Fold accuracy: --- 1.0 ---\n",
      "INFO:hcga.analysis:Fold accuracy: --- 1.0 ---\n",
      "INFO:hcga.analysis:Fold accuracy: --- 1.0 ---\n",
      "INFO:hcga.analysis:Fold accuracy: --- 0.9 ---\n",
      "INFO:hcga.analysis:Fold accuracy: --- 1.0 ---\n",
      "INFO:hcga.analysis:Accuracy: 0.96 +/- 0.049\n",
      "INFO:hcga.analysis:Now using a reduced set of 100 features with < 0.9 correlation.\n",
      "INFO:hcga.analysis:Fold accuracy: --- 1.0 ---\n",
      "INFO:hcga.analysis:Fold accuracy: --- 1.0 ---\n",
      "INFO:hcga.analysis:Fold accuracy: --- 0.9 ---\n",
      "INFO:hcga.analysis:Fold accuracy: --- 1.0 ---\n",
      "INFO:hcga.analysis:Fold accuracy: --- 1.0 ---\n",
      "INFO:hcga.analysis:Fold accuracy: --- 1.0 ---\n",
      "INFO:hcga.analysis:Fold accuracy: --- 1.0 ---\n",
      "INFO:hcga.analysis:Fold accuracy: --- 1.0 ---\n",
      "INFO:hcga.analysis:Fold accuracy: --- 0.9 ---\n",
      "INFO:hcga.analysis:Fold accuracy: --- 1.0 ---\n",
      "INFO:hcga.analysis:Accuracy with reduced set: 0.98 +/- 0.04\n",
      "INFO:hcga.analysis:Fitting model to all data\n"
     ]
    }
   ],
   "source": [
    "h.analyse_features(save_model=True,plot=False, results_folder='./results/test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct synthetic data with no labels\n",
    "\n",
    "Creating synthetic data constructed in the same way as the training data but without labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this cell is generating some synthetic graph dataset with node features. \n",
    "\n",
    "\n",
    "#defining limits on number of nodes\n",
    "n_min = 20\n",
    "n_max = 50\n",
    "\n",
    "#number of graphs\n",
    "num_g = 20\n",
    "\n",
    "# number of node features - in this example I will generate random node features that aren't useful for classifcation\n",
    "n_nf = 3\n",
    "\n",
    "\n",
    "# empty list of graphs and labels\n",
    "graphs = []\n",
    "node_features = []\n",
    "\n",
    "\n",
    "# setting limits on probability of edge existing for random graphs\n",
    "p_min = 0.1\n",
    "p_max = 0.5\n",
    "\n",
    "# adding 50 random graphs (label 0)\n",
    "for i in range(int(num_g/2)):\n",
    "    rand_n = np.random.randint(n_min,n_max)\n",
    "    rand_p = np.random.randint(int(p_min*100),int(p_max*100))/100   \n",
    "    \n",
    "    g = nx.fast_gnp_random_graph(rand_n,rand_p)    \n",
    "    g.label = 0\n",
    "\n",
    "    node_feat_matrix = np.random.random((rand_n,n_nf))\n",
    "\n",
    "    for i,node in enumerate(g.nodes):\n",
    "        g.nodes[node]['features'] = node_feat_matrix[i,:]\n",
    "    \n",
    "    #graphs.append(g)\n",
    "    \n",
    "    graphs.append(nx.to_numpy_array(g)*2)\n",
    "    \n",
    "    node_features.append(node_feat_matrix)\n",
    "    \n",
    "\n",
    "# setting limits on number of edges to add per node\n",
    "m_min = 1\n",
    "m_max = 5\n",
    "\n",
    "# adding 50  powerlaw cluster graphs (label 1)\n",
    "for i in range(int(num_g/2)):\n",
    "    rand_n = np.random.randint(n_min,n_max)\n",
    "    rand_p = np.random.randint(int(p_min*100),int(p_max*100))/100   \n",
    "    rand_m = np.random.randint(m_min,m_max)\n",
    "    \n",
    "    g = nx.powerlaw_cluster_graph(rand_n, rand_m, rand_p)\n",
    "    g.label = 1\n",
    "\n",
    "    node_feat_matrix = np.random.random((rand_n,n_nf))\n",
    "    \n",
    "    for i,node in enumerate(g.nodes):\n",
    "        g.nodes[node]['features'] = node_feat_matrix[i,:]\n",
    "        \n",
    "    #graphs.append(g)\n",
    "    \n",
    "    graphs.append(nx.to_numpy_array(g)*2)\n",
    "    node_features.append(node_feat_matrix)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 20 graphs in the unlabelled dataset\n",
      "There are 3 features per node\n"
     ]
    }
   ],
   "source": [
    "# create graph collection object\n",
    "graphs_unlabelled = GraphCollection()\n",
    "graphs_unlabelled.add_graph_list(graphs,node_features) # loaded without the labels\n",
    "\n",
    "# save the unlabelled dataset\n",
    "save_dataset(graphs_unlabelled, 'custom_dataset_classification_unlabelled', folder='./datasets')\n",
    "\n",
    "# perform some sanity checks\n",
    "print('There are {} graphs in the unlabelled dataset'.format(len(graphs_unlabelled.graphs)))\n",
    "print('There are {} features per node'.format(graphs_unlabelled.get_n_node_features()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract and predict unlabelled data using pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:hcga.extraction:Extracting features from 20 graphs (we disabled 0 graphs).\n",
      "INFO:hcga.extraction:Computing features for 20 graphs:\n",
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done   5 tasks      | elapsed:    2.4s\n",
      "[Parallel(n_jobs=4)]: Done  10 tasks      | elapsed:    4.5s\n",
      "[Parallel(n_jobs=4)]: Done  16 out of  20 | elapsed:    7.0s remaining:    1.8s\n",
      "[Parallel(n_jobs=4)]: Done  20 out of  20 | elapsed:    8.4s finished\n",
      "INFO:hcga.extraction:1093 feature extracted.\n"
     ]
    }
   ],
   "source": [
    "# extract features for the secondary dataset with no labels\n",
    "h.load_data('./datasets/custom_dataset_classification_unlabelled.pkl') # set prediction graphs to True\n",
    "h.extract(mode='fast', n_workers=4, timeout=20) # set prediction set to True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:hcga.analysis:1093 total features\n",
      "INFO:hcga.analysis:0 graphs were removed for more than 0.3 fraction of bad features\n",
      "INFO:hcga.analysis:1017 valid features\n",
      "INFO:hcga.analysis:1017 with interpretability 1\n"
     ]
    }
   ],
   "source": [
    "h.analyse_features(plot=False, trained_model='./results/test/fitted_model', results_folder='./results/test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lets look at the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    y_prediction\n",
      "0            0.0\n",
      "1            0.0\n",
      "2            0.0\n",
      "3            0.0\n",
      "4            0.0\n",
      "5            0.0\n",
      "6            0.0\n",
      "7            0.0\n",
      "8            0.0\n",
      "9            0.0\n",
      "10           1.0\n",
      "11           1.0\n",
      "12           1.0\n",
      "13           1.0\n",
      "14           1.0\n",
      "15           1.0\n",
      "16           1.0\n",
      "17           1.0\n",
      "18           1.0\n",
      "19           1.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "predictions = pd.read_csv('./results/test/prediction_results.csv',index_col=0)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
