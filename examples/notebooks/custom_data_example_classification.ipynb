{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Example: synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import scipy as sc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this cell is generating some synthetic graph dataset with node features. \n",
    "\n",
    "\n",
    "#defining limits on number of nodes\n",
    "n_min = 20\n",
    "n_max = 50\n",
    "\n",
    "#number of graphs\n",
    "num_g = 100\n",
    "\n",
    "# number of node features - in this example I will generate random node features that aren't useful for classifcation\n",
    "n_nf = 3\n",
    "\n",
    "\n",
    "# empty list of graphs and labels\n",
    "graphs = []\n",
    "labels = []\n",
    "node_features = []\n",
    "\n",
    "\n",
    "# setting limits on probability of edge existing for random graphs\n",
    "p_min = 0.1\n",
    "p_max = 0.5\n",
    "\n",
    "# adding 50 random graphs (label 0)\n",
    "for i in range(int(num_g/2)):\n",
    "    rand_n = np.random.randint(n_min,n_max)\n",
    "    rand_p = np.random.randint(int(p_min*100),int(p_max*100))/100   \n",
    "    \n",
    "    g = nx.fast_gnp_random_graph(rand_n,rand_p)    \n",
    "    node_feat_matrix = np.random.random((rand_n,n_nf))\n",
    "    \n",
    "    graphs.append(nx.to_numpy_array(g))\n",
    "    node_features.append(node_feat_matrix)\n",
    "    \n",
    "    labels.append(0)\n",
    "\n",
    "# setting limits on number of edges to add per node\n",
    "m_min = 1\n",
    "m_max = 5\n",
    "\n",
    "# adding 50  powerlaw cluster graphs (label 1)\n",
    "for i in range(int(num_g/2)):\n",
    "    rand_n = np.random.randint(n_min,n_max)\n",
    "    rand_p = np.random.randint(int(p_min*100),int(p_max*100))/100   \n",
    "    rand_m = np.random.randint(m_min,m_max)\n",
    "    \n",
    "    g = nx.powerlaw_cluster_graph(rand_n, rand_m, rand_p)\n",
    "    node_feat_matrix = np.random.random((rand_n,n_nf))\n",
    "    \n",
    "    graphs.append(nx.to_numpy_array(g))\n",
    "    node_features.append(node_feat_matrix)\n",
    "\n",
    "    labels.append(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load graphs into graph object\n",
    "\n",
    "We now have three lists of length 100. The graphs list is composed of numpy arrays that represent the adjacency matrix of the graph. The node features list is composed of numpy arrays that contain the node information for each graph. The labels list is a list of integers that corresponds to the class label for each graph.\n",
    "\n",
    "The next step is to take this data and convert it into an appropriate format for hcga."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting this data into the format required for hcga\n",
    "\n",
    "from hcga.graph import Graph, GraphCollection\n",
    "\n",
    "# create graph collection object\n",
    "g_c = GraphCollection()\n",
    "\n",
    "# add graphs, node features and labels to the object\n",
    "g_c.add_graph_list(graphs,node_features,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 100 graphs\n",
      "There are 3 features per node\n"
     ]
    }
   ],
   "source": [
    "# perform some sanity checks\n",
    "\n",
    "print('There are {} graphs'.format(len(g_c.graphs)))\n",
    "print('There are {} features per node'.format(g_c.get_n_node_features()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can save this if we want to and run everything from the command line\n",
    "from hcga.io import save_dataset\n",
    "\n",
    "save_dataset(g_c, 'custom_dataset_classification', folder='./datasets/custom_dataset_classification')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting features\n",
    "\n",
    "We have now produced a pickle dataset of your own custom data. We can now run the feature extraction from the command line using the following commands:\n",
    "\n",
    "hcga extract_features ./datasets/custom_dataset.pkl -m fast -n 4 -sl advanced --timeout 10 \n",
    "\n",
    "\n",
    "Alternatively,we could import the Hcga class and run the feature extraction and analysis from within the notebook. We will do this below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import hcga object\n",
    "from hcga.hcga import Hcga\n",
    "\n",
    "# define an object\n",
    "h = Hcga()\n",
    "\n",
    "# load previously saved dataset\n",
    "h.load_data('./datasets/custom_dataset_classification/custom_dataset_classification.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:hcga.extraction:Extracting features from 100 graphs (we disabled 0 graphs).\n",
      "INFO:hcga.extraction:Computing features for 100 graphs:\n",
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done   5 tasks      | elapsed:    5.6s\n",
      "[Parallel(n_jobs=4)]: Done  10 tasks      | elapsed:    8.4s\n",
      "[Parallel(n_jobs=4)]: Done  17 tasks      | elapsed:   11.2s\n",
      "[Parallel(n_jobs=4)]: Done  24 tasks      | elapsed:   13.6s\n",
      "[Parallel(n_jobs=4)]: Done  33 tasks      | elapsed:   17.3s\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:   20.8s\n",
      "[Parallel(n_jobs=4)]: Done  53 tasks      | elapsed:   24.8s\n",
      "[Parallel(n_jobs=4)]: Done  64 tasks      | elapsed:   28.3s\n",
      "[Parallel(n_jobs=4)]: Done  77 tasks      | elapsed:   33.3s\n",
      "[Parallel(n_jobs=4)]: Done  90 tasks      | elapsed:   37.9s\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:   41.3s finished\n",
      "INFO:hcga.extraction:1093 feature extracted.\n"
     ]
    }
   ],
   "source": [
    "# extracting all features here\n",
    "h.extract(mode='fast', n_workers=4, timeout=20)\n",
    "\n",
    "# saving all features into a pickle\n",
    "h.save_features('./results/custom_dataset_classification/all_features.pkl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis - classification example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the saved features\n",
    "\n",
    "h.load_features('./results/custom_dataset_classification/all_features.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:hcga.analysis:... Using Xgboost classifier ...\n",
      "INFO:hcga.analysis:1093 total features\n",
      "INFO:hcga.analysis:0 graphs were removed for more than 0.3 fraction of bad features\n",
      "INFO:hcga.analysis:1018 valid features\n",
      "INFO:hcga.analysis:1018 with interpretability 1\n",
      "INFO:hcga.analysis:Counts of graphs/label: \n",
      "0.0    50\n",
      "1.0    50\n",
      "Name: label, dtype: int64\n",
      "INFO:hcga.analysis:Using 10 splits\n",
      "INFO:hcga.analysis:Fold accuracy: --- 1.0 ---\n",
      "INFO:hcga.analysis:Fold accuracy: --- 0.9 ---\n",
      "INFO:hcga.analysis:Fold accuracy: --- 0.9 ---\n",
      "INFO:hcga.analysis:Fold accuracy: --- 0.9 ---\n",
      "INFO:hcga.analysis:Fold accuracy: --- 1.0 ---\n",
      "INFO:hcga.analysis:Fold accuracy: --- 1.0 ---\n",
      "INFO:hcga.analysis:Fold accuracy: --- 1.0 ---\n",
      "INFO:hcga.analysis:Fold accuracy: --- 1.0 ---\n",
      "INFO:hcga.analysis:Fold accuracy: --- 0.9 ---\n",
      "INFO:hcga.analysis:Fold accuracy: --- 1.0 ---\n",
      "INFO:hcga.analysis:Accuracy: 0.96 +/- 0.049\n",
      "INFO:hcga.analysis:Now using a reduced set of 100 features with < 0.9 correlation.\n",
      "INFO:hcga.analysis:Fold accuracy: --- 1.0 ---\n",
      "INFO:hcga.analysis:Fold accuracy: --- 1.0 ---\n",
      "INFO:hcga.analysis:Fold accuracy: --- 0.9 ---\n",
      "INFO:hcga.analysis:Fold accuracy: --- 0.8 ---\n",
      "INFO:hcga.analysis:Fold accuracy: --- 1.0 ---\n",
      "INFO:hcga.analysis:Fold accuracy: --- 1.0 ---\n",
      "INFO:hcga.analysis:Fold accuracy: --- 1.0 ---\n",
      "INFO:hcga.analysis:Fold accuracy: --- 1.0 ---\n",
      "INFO:hcga.analysis:Fold accuracy: --- 0.9 ---\n",
      "INFO:hcga.analysis:Fold accuracy: --- 1.0 ---\n",
      "INFO:hcga.analysis:Accuracy with reduced set: 0.96 +/- 0.066\n",
      "INFO:hcga.plotting:Using reduced features for plotting.\n",
      "INFO:hcga.plotting:Plot bar ranking\n",
      "INFO:hcga.plotting:Plot dot summary\n",
      "INFO:hcga.plotting:Plot feature correlations\n",
      "INFO:hcga.plotting:Plot dendrogram\n",
      "INFO:hcga.plotting:Plot shap violin\n",
      "INFO:hcga.plotting:Plot feature summaries\n"
     ]
    }
   ],
   "source": [
    "# implement a classification analyse of the features\n",
    "\n",
    "h.analyse_features(feature_file='./results/custom_dataset_classification/all_features.pkl',results_folder='./results/custom_dataset_classification')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
